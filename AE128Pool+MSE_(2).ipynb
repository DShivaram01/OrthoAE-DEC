{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q3PKkum4IBF",
        "outputId": "5683fd0b-abcc-4838-fe83-15c6898400ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Clone OrthoLM and ESM (run once per runtime)\n",
        "!git clone https://github.com/ThomasGTHB/OrthoLM.git -q\n",
        "!git clone https://github.com/facebookresearch/esm.git -q\n",
        "\n",
        "import os\n",
        "os.chdir(\"OrthoLM\")  # safer than %cd for Kaggle too\n",
        "os.makedirs(\"Results\", exist_ok=True)\n",
        "\n",
        "# Install packages (REMOVE the literal \"...\" line)\n",
        "!pip -q install biopython matplotlib-venn fair-esm seaborn tqdm joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Imports =====\n",
        "import os, time, json, csv, copy, random, itertools, pickle\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import esm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA  # (unused here but kept for compatibility)\n",
        "from sklearn.metrics import adjusted_mutual_info_score\n",
        "import joblib\n"
      ],
      "metadata": {
        "id": "asPllphU4cdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8AnTd34R4kAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b1b978-f6b6-4f52-d829-f720cd356459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Config =====\n",
        "DATA_ROOT   = \"/content/OrthoLM/Datasets/\"\n",
        "\n",
        "TRAIN_SPEC  = [\"drer_xtro\", \"mmus_hsap\"]   # pooled train\n",
        "TEST_SPEC   = \"pfal_pber\"                  # held-out test\n",
        "\n",
        "MODEL_NAME  = \"esm2_t36_3B_UR50D\"\n",
        "EMB_LAYER   = 36\n",
        "\n",
        "# AE\n",
        "LATENT_DIM  = 128\n",
        "LAMBDA_L1   = 7.5e-4\n",
        "NOISE_SIGMA = 0.0\n",
        "DROPOUT_P   = 0.10\n",
        "LR          = 1e-3\n",
        "WD          = 1e-5\n",
        "BATCH_SIZE  = 512\n",
        "EPOCHS      = 120\n",
        "PATIENCE    = 10\n",
        "\n",
        "# NEW: loss selector (\"mse\" or \"cosine\")\n",
        "LOSS_TYPE: str = \"cosine\"            # <-- flip to \"cosine\" for the second run\n",
        "\n",
        "# KMeans\n",
        "KMEANS_NINIT   = 50\n",
        "KMEANS_MAXITER = 500\n",
        "KMEANS_ALGO    = \"elkan\"\n",
        "RAND_STATE     = 0\n",
        "\n",
        "# Variant: also evaluate a Top-k sparse version of latents\n",
        "USE_TOPK = False\n",
        "TOPK     = 32\n",
        "\n",
        "# Reproducibility & device\n",
        "SEED = 0\n",
        "np.random.seed(SEED); random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "# ===== Experiment folder =====\n",
        "ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "loss_tag = f\"loss-{LOSS_TYPE}\"\n",
        "topk_tag = f\"topk{TOPK}\" if USE_TOPK else \"topk0\"\n",
        "exp_name = (\n",
        "    f\"TRAIN[{'+'.join(TRAIN_SPEC)}]_TEST[{TEST_SPEC}]_\"\n",
        "    f\"{MODEL_NAME}_AE{LATENT_DIM}_L1-{LAMBDA_L1}_SIG{NOISE_SIGMA}_DO{DROPOUT_P}_\"\n",
        "    f\"ninit{KMEANS_NINIT}_iter{KMEANS_MAXITER}_topk{TOPK if USE_TOPK else 0}_{ts}_\"\n",
        "    f\"{MODEL_NAME}_AE{LATENT_DIM}_{loss_tag}_L1-{LAMBDA_L1}_SIG{NOISE_SIGMA}_DO{DROPOUT_P}_\"\n",
        "    f\"ninit{KMEANS_NINIT}_iter{KMEANS_MAXITER}_{topk_tag}_{ts}\"\n",
        ")\n",
        "RESULTS_ROOT = \"/content/drive/MyDrive/ae_ablations/lamdaL1_7.5e-4\"\n",
        "EXP_DIR = os.path.join(RESULTS_ROOT, exp_name)\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "print(\"Saving to:\", EXP_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOTym11H4fnA",
        "outputId": "48389953-2153-45e0-8f40-1a7776fb3abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "Saving to: /content/drive/MyDrive/ae_ablations/lamdaL1_7.5e-4/TRAIN[drer_xtro+mmus_hsap]_TEST[pfal_pber]_esm2_t36_3B_UR50D_AE128_L1-0.00075_SIG0.0_DO0.1_ninit50_iter500_topk0_20250913-144019_esm2_t36_3B_UR50D_AE128_loss-cosine_L1-0.00075_SIG0.0_DO0.1_ninit50_iter500_topk0_20250913-144019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_species_matrix(species, model_name, emb_layer, data_root):\n",
        "    \"\"\"Return:\n",
        "       Xs  : (N, D) float32 mean embeddings\n",
        "       ids : list[str] fasta headers (order matches Xs rows)\n",
        "       meta: list[list[str]] split by '|' → [species_code, protein_code, ..., OG]\n",
        "    \"\"\"\n",
        "    fasta_path = os.path.join(data_root, f\"{species}.fasta\")\n",
        "    emb_dir    = os.path.join(data_root, f\"{species}_emb_{model_name}\")\n",
        "    Xs, ids, meta = [], [], []\n",
        "    for header, _seq in esm.data.read_fasta(fasta_path):\n",
        "        pt = os.path.join(emb_dir, f\"{header}.pt\")\n",
        "        if not os.path.isfile(pt):\n",
        "            continue\n",
        "        embs = torch.load(pt)\n",
        "        Xs.append(embs['mean_representations'][emb_layer])\n",
        "        ids.append(header)\n",
        "        meta.append(header.split('|'))\n",
        "    Xs = torch.stack(Xs, dim=0).numpy().astype(np.float32)\n",
        "    return Xs, ids, meta\n",
        "\n",
        "def count_OGs(prot_meta):\n",
        "    return len(set([p[4] for p in prot_meta]))\n",
        "\n",
        "def save_json(obj, path):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2)\n"
      ],
      "metadata": {
        "id": "pKxLbAC74v9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Pooled train -----\n",
        "Xs_train_list, meta_train_list = [], []\n",
        "for sp in TRAIN_SPEC:\n",
        "    Xs_sp, ids_sp, meta_sp = load_species_matrix(sp, MODEL_NAME, EMB_LAYER, DATA_ROOT)\n",
        "    np.save(os.path.join(EXP_DIR, f\"{sp}_ids.npy\"), np.array(ids_sp, dtype=object))\n",
        "    Xs_train_list.append(Xs_sp)\n",
        "    meta_train_list += meta_sp\n",
        "    print(f\"[train] {sp}: X={Xs_sp.shape}, OGs={count_OGs(meta_sp)}\")\n",
        "\n",
        "Xs_train_pool = np.concatenate(Xs_train_list, axis=0).astype(np.float32)\n",
        "print(\"[train pool]\", Xs_train_pool.shape, \"from\", TRAIN_SPEC)\n",
        "\n",
        "# Fit scaler on pooled train (save it)\n",
        "scaler = StandardScaler().fit(Xs_train_pool)\n",
        "joblib.dump(scaler, os.path.join(EXP_DIR, \"scaler.joblib\"))\n",
        "\n",
        "# ----- Test -----\n",
        "Xs_test, ids_test, meta_test = load_species_matrix(TEST_SPEC, MODEL_NAME, EMB_LAYER, DATA_ROOT)\n",
        "np.save(os.path.join(EXP_DIR, f\"{TEST_SPEC}_ids.npy\"), np.array(ids_test, dtype=object))\n",
        "print(f\"[test] {TEST_SPEC}: X={Xs_test.shape}, OGs={count_OGs(meta_test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIDeu-dH4z9R",
        "outputId": "2152c08b-5e25-43f4-e0b2-b18184b25185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] drer_xtro: X=(61670, 2560), OGs=18374\n",
            "[train] mmus_hsap: X=(45087, 2560), OGs=16659\n",
            "[train pool] (106757, 2560) from ['drer_xtro', 'mmus_hsap']\n",
            "[test] pfal_pber: X=(10263, 2560), OGs=5315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def saving_from_kmeans(Xs_train_pca, prot_names_and_group_train, kmeans):\n",
        "    n_clusters = kmeans.n_clusters\n",
        "    n_samples  = Xs_train_pca.shape[0]\n",
        "    X_labels   = kmeans.labels_\n",
        "    return [n_clusters, n_samples, prot_names_and_group_train, X_labels, kmeans,\n",
        "            \"n_clusters, n_samples, prot_names_and_group_train, X_labels, kmeans\"]\n",
        "\n",
        "def measure_pairwise_performance(saved_results, Xs_train_pca):\n",
        "    n_clusters = saved_results[0]\n",
        "    n_samples  = saved_results[1]\n",
        "    prot_names_and_group_train = saved_results[2]\n",
        "    X_labels   = saved_results[3]\n",
        "    kmeans     = saved_results[4]\n",
        "\n",
        "    X_dist = kmeans.transform(Xs_train_pca)**2\n",
        "    orthologs_naiveSearch = []\n",
        "    orthologs_distanceBasedSearch = []\n",
        "    orthologs_1_to_1 = []\n",
        "\n",
        "    n_species_total = len(list(set([prot[0] for prot in prot_names_and_group_train])))\n",
        "    for cluster in range(n_clusters):\n",
        "        ind_fromCluster = [i for i, x in enumerate(X_labels) if x==cluster]\n",
        "        if len(ind_fromCluster) == 1:\n",
        "            continue\n",
        "\n",
        "        ind_sorted = np.argsort(X_dist[ind_fromCluster, cluster])\n",
        "        all_prots = [prot_names_and_group_train[ind_fromCluster[i]] for i in ind_sorted]\n",
        "        all_specs = [prot_names_and_group_train[ind_fromCluster[i]][0] for i in ind_sorted]\n",
        "\n",
        "        # Naive: all cross-species pairs\n",
        "        if len(list(set(all_specs))) > 1:\n",
        "            for i1 in range(len(all_specs) - 1):\n",
        "                for i2 in range(i1 + 1, len(all_specs)):\n",
        "                    if all_specs[i1] != all_specs[i2]:\n",
        "                        orthologs_naiveSearch.append([all_prots[i1], all_prots[i2]])\n",
        "\n",
        "        # Distance-based: top of cluster + nearest with different species\n",
        "        ind_species2 = 1\n",
        "        if all_specs[0] != all_specs[ind_species2]:\n",
        "            orthologs_distanceBasedSearch.append([all_prots[0], all_prots[ind_species2]])\n",
        "        else:\n",
        "            while ind_species2 < len(all_specs) and all_specs[0] == all_specs[ind_species2]:\n",
        "                ind_species2 += 1\n",
        "                if ind_species2 < len(all_specs) and all_specs[0] != all_specs[ind_species2]:\n",
        "                    orthologs_distanceBasedSearch.append([all_prots[0], all_prots[ind_species2]])\n",
        "                    break\n",
        "\n",
        "        # 1-to-1: one per species in the cluster\n",
        "        if len(all_specs) == len(list(set(all_specs))) == n_species_total:\n",
        "            orthologs_1_to_1.append(all_prots)\n",
        "\n",
        "    return [orthologs_naiveSearch, orthologs_distanceBasedSearch, orthologs_1_to_1]\n",
        "\n",
        "def measure_group_performance(saved_results):\n",
        "    n_clusters = saved_results[0]\n",
        "    n_samples  = saved_results[1]\n",
        "    prot_names_and_group_train = saved_results[2]\n",
        "    X_labels   = saved_results[3]\n",
        "\n",
        "    # groups per cluster\n",
        "    list_groups_in_clusters = [[] for _ in range(n_clusters)]\n",
        "    for i_label in range(n_samples):\n",
        "        list_groups_in_clusters[X_labels[i_label]].append(prot_names_and_group_train[i_label][4])\n",
        "\n",
        "    list_all_groups = list(set([prot[4] for prot in prot_names_and_group_train]))\n",
        "\n",
        "    list_OG_count_in_cluster = np.zeros((len(list_all_groups), n_clusters))\n",
        "    for i_cluster in range(n_clusters):\n",
        "        for group in list_groups_in_clusters[i_cluster]:\n",
        "            list_OG_count_in_cluster[list_all_groups.index(group), i_cluster] += 1\n",
        "    if not (np.sum(list_OG_count_in_cluster) == n_samples):\n",
        "        print(\"Error: missing some sequences in the count matrix\")\n",
        "\n",
        "    # Family completeness\n",
        "    family_complet_stat = 0\n",
        "    for i_group in range(len(list_all_groups)):\n",
        "        family_complet_stat += max(list_OG_count_in_cluster[i_group,:])\n",
        "    family_complet_stat = family_complet_stat / n_samples\n",
        "\n",
        "    # AMI\n",
        "    AMI = adjusted_mutual_info_score([prot[4] for prot in prot_names_and_group_train], X_labels)\n",
        "\n",
        "    # % exact matches\n",
        "    i_count_success_exactMatch = 0\n",
        "    for i_group in range(len(list_all_groups)):\n",
        "        if np.count_nonzero(list_OG_count_in_cluster[i_group,:] == 0) == n_clusters - 1:\n",
        "            i_cluster = np.nonzero(list_OG_count_in_cluster[i_group,:])[0][0]\n",
        "            if np.count_nonzero(list_OG_count_in_cluster[:,i_cluster] == 0) == len(list_all_groups) - 1:\n",
        "                i_count_success_exactMatch += 1\n",
        "    exact_over_total = i_count_success_exactMatch / n_clusters\n",
        "\n",
        "    return [family_complet_stat, AMI, exact_over_total]\n",
        "\n",
        "# utils.py\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def l2_normalize_rows(x: np.ndarray, eps=1e-9):\n",
        "    n = np.linalg.norm(x, axis=1, keepdims=True)\n",
        "    return x / (n + eps)\n",
        "\n",
        "@torch.no_grad()\n",
        "def torch_l2_normalize_rows(x: torch.Tensor, eps=1e-8):\n",
        "    n = x.norm(dim=1, keepdim=True)\n",
        "    return x / (n + eps)\n",
        "\n",
        "# Small helper: cosine reconstruction loss (row-wise)\n",
        "def cosine_recon_loss(x_hat: torch.Tensor, x: torch.Tensor, eps: float = 1e-8):\n",
        "    xh = x_hat / (x_hat.norm(dim=1, keepdim=True) + eps)\n",
        "    xx = x     / (x.norm(dim=1, keepdim=True)     + eps)\n",
        "    # 1 - cos similarity; mean over batch\n",
        "    return (1.0 - (xh * xx).sum(dim=1)).mean()\n",
        "# class CosineReconLoss(torch.nn.Module):\n",
        "#     def __init__(self, eps=1e-8):\n",
        "#         super().__init__()\n",
        "#         self.eps = eps\n",
        "#     def forward(self, x_hat, x):\n",
        "#         xh = x_hat / (x_hat.norm(dim=1, keepdim=True) + self.eps)\n",
        "#         x  = x     / (x.norm(dim=1, keepdim=True)     + self.eps)\n",
        "#         # want high cosine similarity -> minimize (1 - cos)\n",
        "#         return (1.0 - (xh * x).sum(dim=1)).mean()\n",
        "\n",
        "def compute_k_from_token(k_token: str, N: int) -> int:\n",
        "    if isinstance(k_token, int):\n",
        "        return k_token\n",
        "    if isinstance(k_token, str):\n",
        "        if k_token.upper() == \"N2\":\n",
        "            return max(2, N // 2)\n",
        "        raise ValueError(f\"Unknown k token: {k_token}\")\n",
        "    raise ValueError(f\"Unsupported k type: {type(k_token)}\")"
      ],
      "metadata": {
        "id": "JwUr6Uqw41-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- AE (MSE vs Cosine) training cell ---------------------------------------\n",
        "import os, csv, copy, torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# -------------------- choose the loss here -----------------------------------\n",
        "#LOSS_TYPE = \"mse\"      # <-- change to \"cosine\" for the second experiment\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# For this experiment: no extras (as requested)\n",
        "# LAMBDA_L1   = 0.0      # force OFF\n",
        "NOISE_SIGMA = 0.0      # force OFF\n",
        "\n",
        "\n",
        "\n",
        "class AE(nn.Module):\n",
        "    def __init__(self, in_dim, latent_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(512, 128),   nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(128, latent_dim)\n",
        "        )\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 512),        nn.ReLU(),\n",
        "            nn.Linear(512, in_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        z = self.enc(x)\n",
        "        x_hat = self.dec(z)\n",
        "        return x_hat, z\n",
        "\n",
        "# -------------------- data split & loaders -----------------------------------\n",
        "# expects: scaler, Xs_train_pool, BATCH_SIZE, device, LATENT_DIM, LR, WD, EPOCHS, PATIENCE, DROPOUT_P, EXP_DIR\n",
        "Xs_train_scaled = scaler.transform(Xs_train_pool).astype(np.float32)\n",
        "X_tensor = torch.from_numpy(Xs_train_scaled)\n",
        "perm = torch.randperm(X_tensor.size(0))\n",
        "n_val = int(0.10 * len(perm))\n",
        "val_idx, train_idx = perm[:n_val], perm[n_val:]\n",
        "\n",
        "tr_dl = DataLoader(TensorDataset(X_tensor[train_idx]), batch_size=BATCH_SIZE, shuffle=True)\n",
        "va_dl = DataLoader(TensorDataset(X_tensor[val_idx]),   batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# -------------------- model/optim/loss ---------------------------------------\n",
        "ae  = AE(Xs_train_scaled.shape[1], LATENT_DIM, dropout=DROPOUT_P).to(device)\n",
        "opt = torch.optim.Adam(ae.parameters(), lr=LR, weight_decay=WD)\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "best_val = float('inf'); best_state = None; no_improve = 0\n",
        "log_rows = []\n",
        "\n",
        "print(f\"[AE] training start (loss={LOSS_TYPE}, latent={LATENT_DIM}, L1={LAMBDA_L1}, noise={NOISE_SIGMA})\")\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    ae.train(); tr_loss = tr_recon = 0.0\n",
        "    for (xb,) in tr_dl:\n",
        "        xb = xb.to(device)\n",
        "\n",
        "        # no noise for this experiment (kept line for clarity)\n",
        "        noisy = xb + NOISE_SIGMA * torch.randn_like(xb) if NOISE_SIGMA > 0 else xb\n",
        "\n",
        "        x_hat, z = ae(noisy)\n",
        "        if LOSS_TYPE == \"mse\":\n",
        "            recon = mse(x_hat, xb)\n",
        "        elif LOSS_TYPE == \"cosine\":\n",
        "            recon = cosine_recon_loss(x_hat, xb)\n",
        "        else:\n",
        "            raise ValueError(\"LOSS_TYPE must be 'mse' or 'cosine'\")\n",
        "\n",
        "        # # no L1 for this experiment\n",
        "        # loss = recon\n",
        "        spars = z.abs().mean()              # ← L1 on latent activations\n",
        "        loss  = recon + LAMBDA_L1 * spars   # ← APPLY λ here\n",
        "\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "\n",
        "        tr_loss  += loss.item() * xb.size(0)\n",
        "        tr_recon += recon.item() * xb.size(0)\n",
        "\n",
        "    tr_loss  /= len(tr_dl.dataset)\n",
        "    tr_recon /= len(tr_dl.dataset)\n",
        "\n",
        "    # ---------------- validation ----------------\n",
        "    ae.eval(); va_loss = va_recon = 0.0\n",
        "    with torch.no_grad():\n",
        "        for (xb,) in va_dl:\n",
        "            xb = xb.to(device)\n",
        "            x_hat, z = ae(xb)\n",
        "            if LOSS_TYPE == \"mse\":\n",
        "                recon = mse(x_hat, xb)\n",
        "            else:\n",
        "                recon = cosine_recon_loss(x_hat, xb)\n",
        "            spars = z.abs().mean()\n",
        "            loss  = recon + LAMBDA_L1 * spars\n",
        "\n",
        "            va_loss  += loss.item()  * xb.size(0)\n",
        "            va_recon += recon.item() * xb.size(0)\n",
        "\n",
        "    va_loss  /= len(va_dl.dataset)\n",
        "    va_recon /= len(va_dl.dataset)\n",
        "\n",
        "    tag = \"\"\n",
        "    if va_loss < best_val - 1e-6:\n",
        "        best_val = va_loss\n",
        "        best_state = {\"ae\": copy.deepcopy(ae.state_dict())}\n",
        "        tag = \"**best**\"; no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "\n",
        "    print(f\"[AE] ep{ep:03d}  train: total={tr_loss:.5f} ({LOSS_TYPE}={tr_recon:.5f}) | \"\n",
        "      f\"val: total={va_loss:.5f} ({LOSS_TYPE}={va_recon:.5f}) | {tag}\")\n",
        "    log_rows.append([ep, tr_loss, tr_recon, va_loss, va_recon])\n",
        "    if no_improve >= PATIENCE:\n",
        "        break\n",
        "\n",
        "# -------------------- save best weights + log (tagged by loss) ---------------\n",
        "loss_tag = LOSS_TYPE.lower()\n",
        "lam_tag  = f\"lam{LAMBDA_L1:g}\"\n",
        "\n",
        "if best_state is not None:\n",
        "    torch.save(best_state,       os.path.join(EXP_DIR, f\"ae_checkpoint_{loss_tag}_{lam_tag}.pt\"))\n",
        "    torch.save(best_state[\"ae\"], os.path.join(EXP_DIR, f\"ae_state_{loss_tag}_{lam_tag}.pth\"))\n",
        "\n",
        "with open(os.path.join(EXP_DIR, f\"trainlog_{loss_tag}_{lam_tag}.csv\"), \"w\", newline=\"\") as f:\n",
        "    cw = csv.writer(f)\n",
        "    cw.writerow([\"epoch\",\"tr_total\",f\"tr_{loss_tag}\",\"va_total\",f\"va_{loss_tag}\"])\n",
        "    cw.writerows(log_rows)\n",
        "\n",
        "print(f\"[saved] AE + logs (loss={loss_tag}) → {EXP_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJsxMsl65MZN",
        "outputId": "ff3ec45c-11c8-499c-92a9-56dd27a0484c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] training start (loss=cosine, latent=128, L1=0.00075, noise=0.0)\n",
            "[AE] ep001  train: total=0.39104 (cosine=0.39003) | val: total=0.28412 (cosine=0.28323) | **best**\n",
            "[AE] ep002  train: total=0.27072 (cosine=0.26981) | val: total=0.23410 (cosine=0.23324) | **best**\n",
            "[AE] ep003  train: total=0.23971 (cosine=0.23881) | val: total=0.21124 (cosine=0.21039) | **best**\n",
            "[AE] ep004  train: total=0.22421 (cosine=0.22334) | val: total=0.19829 (cosine=0.19747) | **best**\n",
            "[AE] ep005  train: total=0.21448 (cosine=0.21362) | val: total=0.19172 (cosine=0.19091) | **best**\n",
            "[AE] ep006  train: total=0.20791 (cosine=0.20707) | val: total=0.18463 (cosine=0.18384) | **best**\n",
            "[AE] ep007  train: total=0.20338 (cosine=0.20255) | val: total=0.18083 (cosine=0.18006) | **best**\n",
            "[AE] ep008  train: total=0.20000 (cosine=0.19918) | val: total=0.17800 (cosine=0.17724) | **best**\n",
            "[AE] ep009  train: total=0.19704 (cosine=0.19624) | val: total=0.17599 (cosine=0.17524) | **best**\n",
            "[AE] ep010  train: total=0.19476 (cosine=0.19397) | val: total=0.17304 (cosine=0.17229) | **best**\n",
            "[AE] ep011  train: total=0.19257 (cosine=0.19179) | val: total=0.17174 (cosine=0.17101) | **best**\n",
            "[AE] ep012  train: total=0.19066 (cosine=0.18989) | val: total=0.16952 (cosine=0.16879) | **best**\n",
            "[AE] ep013  train: total=0.18919 (cosine=0.18841) | val: total=0.16816 (cosine=0.16743) | **best**\n",
            "[AE] ep014  train: total=0.18764 (cosine=0.18688) | val: total=0.16644 (cosine=0.16572) | **best**\n",
            "[AE] ep015  train: total=0.18627 (cosine=0.18551) | val: total=0.16587 (cosine=0.16516) | **best**\n",
            "[AE] ep016  train: total=0.18533 (cosine=0.18457) | val: total=0.16458 (cosine=0.16386) | **best**\n",
            "[AE] ep017  train: total=0.18406 (cosine=0.18331) | val: total=0.16356 (cosine=0.16286) | **best**\n",
            "[AE] ep018  train: total=0.18302 (cosine=0.18227) | val: total=0.16288 (cosine=0.16218) | **best**\n",
            "[AE] ep019  train: total=0.18205 (cosine=0.18130) | val: total=0.16181 (cosine=0.16110) | **best**\n",
            "[AE] ep020  train: total=0.18110 (cosine=0.18035) | val: total=0.16043 (cosine=0.15972) | **best**\n",
            "[AE] ep021  train: total=0.18021 (cosine=0.17947) | val: total=0.15966 (cosine=0.15896) | **best**\n",
            "[AE] ep022  train: total=0.17953 (cosine=0.17878) | val: total=0.15892 (cosine=0.15822) | **best**\n",
            "[AE] ep023  train: total=0.17861 (cosine=0.17787) | val: total=0.15816 (cosine=0.15746) | **best**\n",
            "[AE] ep024  train: total=0.17811 (cosine=0.17737) | val: total=0.15755 (cosine=0.15685) | **best**\n",
            "[AE] ep025  train: total=0.17737 (cosine=0.17663) | val: total=0.15800 (cosine=0.15731) | \n",
            "[AE] ep026  train: total=0.17663 (cosine=0.17589) | val: total=0.15661 (cosine=0.15591) | **best**\n",
            "[AE] ep027  train: total=0.17598 (cosine=0.17525) | val: total=0.15593 (cosine=0.15523) | **best**\n",
            "[AE] ep028  train: total=0.17544 (cosine=0.17470) | val: total=0.15579 (cosine=0.15509) | **best**\n",
            "[AE] ep029  train: total=0.17470 (cosine=0.17397) | val: total=0.15468 (cosine=0.15398) | **best**\n",
            "[AE] ep030  train: total=0.17426 (cosine=0.17352) | val: total=0.15441 (cosine=0.15372) | **best**\n",
            "[AE] ep031  train: total=0.17355 (cosine=0.17282) | val: total=0.15420 (cosine=0.15351) | **best**\n",
            "[AE] ep032  train: total=0.17304 (cosine=0.17230) | val: total=0.15320 (cosine=0.15251) | **best**\n",
            "[AE] ep033  train: total=0.17260 (cosine=0.17187) | val: total=0.15323 (cosine=0.15254) | \n",
            "[AE] ep034  train: total=0.17198 (cosine=0.17124) | val: total=0.15274 (cosine=0.15204) | **best**\n",
            "[AE] ep035  train: total=0.17147 (cosine=0.17074) | val: total=0.15202 (cosine=0.15134) | **best**\n",
            "[AE] ep036  train: total=0.17095 (cosine=0.17022) | val: total=0.15206 (cosine=0.15137) | \n",
            "[AE] ep037  train: total=0.17046 (cosine=0.16973) | val: total=0.15136 (cosine=0.15067) | **best**\n",
            "[AE] ep038  train: total=0.17021 (cosine=0.16948) | val: total=0.15067 (cosine=0.14998) | **best**\n",
            "[AE] ep039  train: total=0.16956 (cosine=0.16882) | val: total=0.15026 (cosine=0.14957) | **best**\n",
            "[AE] ep040  train: total=0.16914 (cosine=0.16841) | val: total=0.15025 (cosine=0.14956) | **best**\n",
            "[AE] ep041  train: total=0.16875 (cosine=0.16802) | val: total=0.14988 (cosine=0.14920) | **best**\n",
            "[AE] ep042  train: total=0.16833 (cosine=0.16760) | val: total=0.14900 (cosine=0.14832) | **best**\n",
            "[AE] ep043  train: total=0.16780 (cosine=0.16707) | val: total=0.14879 (cosine=0.14811) | **best**\n",
            "[AE] ep044  train: total=0.16749 (cosine=0.16675) | val: total=0.14853 (cosine=0.14785) | **best**\n",
            "[AE] ep045  train: total=0.16702 (cosine=0.16629) | val: total=0.14863 (cosine=0.14795) | \n",
            "[AE] ep046  train: total=0.16678 (cosine=0.16605) | val: total=0.14834 (cosine=0.14765) | **best**\n",
            "[AE] ep047  train: total=0.16620 (cosine=0.16547) | val: total=0.14765 (cosine=0.14697) | **best**\n",
            "[AE] ep048  train: total=0.16587 (cosine=0.16514) | val: total=0.14719 (cosine=0.14651) | **best**\n",
            "[AE] ep049  train: total=0.16554 (cosine=0.16481) | val: total=0.14651 (cosine=0.14582) | **best**\n",
            "[AE] ep050  train: total=0.16519 (cosine=0.16446) | val: total=0.14678 (cosine=0.14611) | \n",
            "[AE] ep051  train: total=0.16488 (cosine=0.16415) | val: total=0.14655 (cosine=0.14587) | \n",
            "[AE] ep052  train: total=0.16437 (cosine=0.16364) | val: total=0.14606 (cosine=0.14537) | **best**\n",
            "[AE] ep053  train: total=0.16402 (cosine=0.16329) | val: total=0.14583 (cosine=0.14515) | **best**\n",
            "[AE] ep054  train: total=0.16369 (cosine=0.16296) | val: total=0.14559 (cosine=0.14491) | **best**\n",
            "[AE] ep055  train: total=0.16342 (cosine=0.16270) | val: total=0.14512 (cosine=0.14444) | **best**\n",
            "[AE] ep056  train: total=0.16313 (cosine=0.16240) | val: total=0.14457 (cosine=0.14389) | **best**\n",
            "[AE] ep057  train: total=0.16277 (cosine=0.16204) | val: total=0.14423 (cosine=0.14355) | **best**\n",
            "[AE] ep058  train: total=0.16254 (cosine=0.16181) | val: total=0.14468 (cosine=0.14400) | \n",
            "[AE] ep059  train: total=0.16231 (cosine=0.16159) | val: total=0.14403 (cosine=0.14334) | **best**\n",
            "[AE] ep060  train: total=0.16185 (cosine=0.16112) | val: total=0.14414 (cosine=0.14346) | \n",
            "[AE] ep061  train: total=0.16157 (cosine=0.16085) | val: total=0.14356 (cosine=0.14288) | **best**\n",
            "[AE] ep062  train: total=0.16138 (cosine=0.16066) | val: total=0.14348 (cosine=0.14280) | **best**\n",
            "[AE] ep063  train: total=0.16120 (cosine=0.16047) | val: total=0.14329 (cosine=0.14261) | **best**\n",
            "[AE] ep064  train: total=0.16085 (cosine=0.16012) | val: total=0.14309 (cosine=0.14242) | **best**\n",
            "[AE] ep065  train: total=0.16078 (cosine=0.16005) | val: total=0.14340 (cosine=0.14273) | \n",
            "[AE] ep066  train: total=0.16042 (cosine=0.15969) | val: total=0.14308 (cosine=0.14240) | **best**\n",
            "[AE] ep067  train: total=0.16020 (cosine=0.15948) | val: total=0.14233 (cosine=0.14165) | **best**\n",
            "[AE] ep068  train: total=0.15999 (cosine=0.15927) | val: total=0.14202 (cosine=0.14135) | **best**\n",
            "[AE] ep069  train: total=0.15975 (cosine=0.15902) | val: total=0.14249 (cosine=0.14181) | \n",
            "[AE] ep070  train: total=0.15958 (cosine=0.15886) | val: total=0.14166 (cosine=0.14098) | **best**\n",
            "[AE] ep071  train: total=0.15938 (cosine=0.15865) | val: total=0.14185 (cosine=0.14117) | \n",
            "[AE] ep072  train: total=0.15906 (cosine=0.15834) | val: total=0.14184 (cosine=0.14116) | \n",
            "[AE] ep073  train: total=0.15896 (cosine=0.15824) | val: total=0.14155 (cosine=0.14088) | **best**\n",
            "[AE] ep074  train: total=0.15865 (cosine=0.15793) | val: total=0.14120 (cosine=0.14052) | **best**\n",
            "[AE] ep075  train: total=0.15835 (cosine=0.15763) | val: total=0.14164 (cosine=0.14098) | \n",
            "[AE] ep076  train: total=0.15831 (cosine=0.15759) | val: total=0.14105 (cosine=0.14038) | **best**\n",
            "[AE] ep077  train: total=0.15815 (cosine=0.15743) | val: total=0.14069 (cosine=0.14001) | **best**\n",
            "[AE] ep078  train: total=0.15762 (cosine=0.15690) | val: total=0.14099 (cosine=0.14032) | \n",
            "[AE] ep079  train: total=0.15770 (cosine=0.15698) | val: total=0.14056 (cosine=0.13988) | **best**\n",
            "[AE] ep080  train: total=0.15724 (cosine=0.15652) | val: total=0.14070 (cosine=0.14003) | \n",
            "[AE] ep081  train: total=0.15719 (cosine=0.15647) | val: total=0.14060 (cosine=0.13994) | \n",
            "[AE] ep082  train: total=0.15703 (cosine=0.15631) | val: total=0.14047 (cosine=0.13981) | **best**\n",
            "[AE] ep083  train: total=0.15682 (cosine=0.15610) | val: total=0.14009 (cosine=0.13942) | **best**\n",
            "[AE] ep084  train: total=0.15667 (cosine=0.15596) | val: total=0.14009 (cosine=0.13942) | \n",
            "[AE] ep085  train: total=0.15649 (cosine=0.15577) | val: total=0.13987 (cosine=0.13920) | **best**\n",
            "[AE] ep086  train: total=0.15641 (cosine=0.15569) | val: total=0.13965 (cosine=0.13898) | **best**\n",
            "[AE] ep087  train: total=0.15619 (cosine=0.15547) | val: total=0.13961 (cosine=0.13894) | **best**\n",
            "[AE] ep088  train: total=0.15598 (cosine=0.15526) | val: total=0.13963 (cosine=0.13896) | \n",
            "[AE] ep089  train: total=0.15593 (cosine=0.15521) | val: total=0.13959 (cosine=0.13892) | **best**\n",
            "[AE] ep090  train: total=0.15577 (cosine=0.15505) | val: total=0.13934 (cosine=0.13867) | **best**\n",
            "[AE] ep091  train: total=0.15571 (cosine=0.15499) | val: total=0.13908 (cosine=0.13841) | **best**\n",
            "[AE] ep092  train: total=0.15555 (cosine=0.15484) | val: total=0.13885 (cosine=0.13818) | **best**\n",
            "[AE] ep093  train: total=0.15526 (cosine=0.15455) | val: total=0.13902 (cosine=0.13836) | \n",
            "[AE] ep094  train: total=0.15534 (cosine=0.15462) | val: total=0.13929 (cosine=0.13863) | \n",
            "[AE] ep095  train: total=0.15513 (cosine=0.15441) | val: total=0.13880 (cosine=0.13814) | **best**\n",
            "[AE] ep096  train: total=0.15489 (cosine=0.15417) | val: total=0.13881 (cosine=0.13815) | \n",
            "[AE] ep097  train: total=0.15497 (cosine=0.15426) | val: total=0.13859 (cosine=0.13793) | **best**\n",
            "[AE] ep098  train: total=0.15490 (cosine=0.15418) | val: total=0.13887 (cosine=0.13821) | \n",
            "[AE] ep099  train: total=0.15474 (cosine=0.15403) | val: total=0.13848 (cosine=0.13782) | **best**\n",
            "[AE] ep100  train: total=0.15430 (cosine=0.15358) | val: total=0.13876 (cosine=0.13810) | \n",
            "[AE] ep101  train: total=0.15441 (cosine=0.15370) | val: total=0.13854 (cosine=0.13788) | \n",
            "[AE] ep102  train: total=0.15443 (cosine=0.15372) | val: total=0.13837 (cosine=0.13771) | **best**\n",
            "[AE] ep103  train: total=0.15429 (cosine=0.15358) | val: total=0.13814 (cosine=0.13748) | **best**\n",
            "[AE] ep104  train: total=0.15417 (cosine=0.15346) | val: total=0.13756 (cosine=0.13690) | **best**\n",
            "[AE] ep105  train: total=0.15406 (cosine=0.15335) | val: total=0.13782 (cosine=0.13716) | \n",
            "[AE] ep106  train: total=0.15386 (cosine=0.15315) | val: total=0.13836 (cosine=0.13771) | \n",
            "[AE] ep107  train: total=0.15385 (cosine=0.15314) | val: total=0.13773 (cosine=0.13707) | \n",
            "[AE] ep108  train: total=0.15368 (cosine=0.15297) | val: total=0.13750 (cosine=0.13684) | **best**\n",
            "[AE] ep109  train: total=0.15373 (cosine=0.15302) | val: total=0.13709 (cosine=0.13644) | **best**\n",
            "[AE] ep110  train: total=0.15339 (cosine=0.15269) | val: total=0.13758 (cosine=0.13693) | \n",
            "[AE] ep111  train: total=0.15339 (cosine=0.15268) | val: total=0.13782 (cosine=0.13717) | \n",
            "[AE] ep112  train: total=0.15336 (cosine=0.15265) | val: total=0.13760 (cosine=0.13695) | \n",
            "[AE] ep113  train: total=0.15321 (cosine=0.15251) | val: total=0.13716 (cosine=0.13651) | \n",
            "[AE] ep114  train: total=0.15306 (cosine=0.15236) | val: total=0.13770 (cosine=0.13705) | \n",
            "[AE] ep115  train: total=0.15308 (cosine=0.15237) | val: total=0.13734 (cosine=0.13668) | \n",
            "[AE] ep116  train: total=0.15285 (cosine=0.15215) | val: total=0.13748 (cosine=0.13683) | \n",
            "[AE] ep117  train: total=0.15294 (cosine=0.15224) | val: total=0.13729 (cosine=0.13664) | \n",
            "[AE] ep118  train: total=0.15282 (cosine=0.15212) | val: total=0.13707 (cosine=0.13641) | **best**\n",
            "[AE] ep119  train: total=0.15265 (cosine=0.15194) | val: total=0.13714 (cosine=0.13650) | \n",
            "[AE] ep120  train: total=0.15253 (cosine=0.15183) | val: total=0.13662 (cosine=0.13598) | **best**\n",
            "[saved] AE + logs (loss=cosine) → /content/drive/MyDrive/ae_ablations/lamdaL1_7.5e-4/TRAIN[drer_xtro+mmus_hsap]_TEST[pfal_pber]_esm2_t36_3B_UR50D_AE128_L1-0.00075_SIG0.0_DO0.1_ninit50_iter500_topk0_20250913-144019_esm2_t36_3B_UR50D_AE128_loss-cosine_L1-0.00075_SIG0.0_DO0.1_ninit50_iter500_topk0_20250913-144019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Try to restore from file if present\n",
        "ckpt_path = os.path.join(EXP_DIR, f\"ae_checkpoint_{LOSS_TYPE}.pt\")\n",
        "if os.path.exists(ckpt_path):\n",
        "    if 'ae' not in globals():\n",
        "        # re-create the AE with the same dims/hparams you trained\n",
        "        ae = AE(Xs_train_pool.shape[1], LATENT_DIM, dropout=DROPOUT_P).to(device)\n",
        "    state_obj = torch.load(ckpt_path, map_location=device)\n",
        "    ae.load_state_dict(state_obj[\"ae\"])\n",
        "    best_state = state_obj\n",
        "\n",
        "# restore best\n",
        "if best_state is not None:\n",
        "    ae.load_state_dict(best_state[\"ae\"])\n",
        "ae.eval()\n",
        "\n",
        "def encode_latents(ae, scaler, X, batch_size=512):\n",
        "    Xs = scaler.transform(X).astype(np.float32)\n",
        "    Zs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, Xs.shape[0], batch_size):\n",
        "            xb = torch.from_numpy(Xs[i:i+batch_size]).to(device)\n",
        "            _, z = ae(xb)\n",
        "            Zs.append(z.cpu().numpy())\n",
        "    Z = np.vstack(Zs).astype(np.float32)\n",
        "    return Z\n",
        "\n",
        "def topk_sparsify(Z, k):\n",
        "    Zs = Z.copy()\n",
        "    # zero all but top-k |values| per row\n",
        "    idx = np.argpartition(np.abs(Zs), -k, axis=1)[:, :-k]\n",
        "    rows = np.arange(Zs.shape[0])[:, None]\n",
        "    Zs[rows, idx] = 0.0\n",
        "    return Zs\n",
        "\n",
        "# plain latents\n",
        "Z_plain = encode_latents(ae, scaler, Xs_test)        # produces (N, latent_dim)\n",
        "Z_plain = Z_plain / (np.linalg.norm(Z_plain, axis=1, keepdims=True) + 1e-9)\n",
        "\n",
        "lat_fn = f\"{TEST_SPEC}_{MODEL_NAME}_AE{LATENT_DIM}_{loss_tag}_{lam_tag}_latents_plain.npy\"\n",
        "np.save(os.path.join(EXP_DIR, lat_fn), Z_plain)\n",
        "\n",
        "# # top-k variant\n",
        "# if USE_TOPK:\n",
        "#     Z_topk = topk_sparsify(Z_plain, TOPK)\n",
        "#     Z_topk = Z_topk / (np.linalg.norm(Z_topk, axis=1, keepdims=True) + 1e-9)\n",
        "#     np.save(os.path.join(EXP_DIR, f\"{TEST_SPEC}_{MODEL_NAME}_AE{LATENT_DIM}_{LOSS_TYPE}_latents_topk{TOPK}.npy\"), Z_topk)\n",
        "\n",
        "print(\"[saved] latents (plain and top-k)\" if USE_TOPK else \"[saved] latents (plain)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIEkcdsG5ggM",
        "outputId": "5b541f9e-ea63-4f0a-c620-2261b6c3bee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[saved] latents (plain)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_OGs(meta_list):\n",
        "    # meta entries are like: [species_code, protein_code, ..., OG_id] -> OG_id at index 4\n",
        "    return len(set([m[4] for m in meta_list]))\n",
        "# meta alias (kept name consistent with your original code)\n",
        "prot_names_and_group_test = meta_test\n",
        "\n",
        "def compute_k_list(meta, N):\n",
        "    ogs = count_OGs(meta)\n",
        "    ks = [N//2] # , ogs, max(2, ogs-500)\n",
        "    # unique+in-range\n",
        "    seen=set(); out=[]\n",
        "    for k in ks:\n",
        "        if 1 < k <= N and k not in seen:\n",
        "            seen.add(k); out.append(k)\n",
        "    return out, ogs\n",
        "\n",
        "def run_kmeans_eval(X_feat, prot_meta, variant_tag):\n",
        "    X_feat = X_feat.astype(np.float32, copy=False)\n",
        "    N = X_feat.shape[0]\n",
        "    k_list, ogs = compute_k_list(prot_meta, N)\n",
        "    print(f\"[{variant_tag}] N={N}, #OGs={ogs}, ks={k_list}\")\n",
        "\n",
        "    kmeans_saving = {\"Xs_train_pca\": X_feat}   # name kept for compatibility\n",
        "    rows = []\n",
        "\n",
        "    for k in k_list:\n",
        "        print(f\"  k={k}\")\n",
        "        t0 = time.time()\n",
        "        km = KMeans(n_clusters=k, n_init=KMEANS_NINIT, max_iter=KMEANS_MAXITER,\n",
        "                    algorithm=KMEANS_ALGO, random_state=RAND_STATE).fit(X_feat)\n",
        "        elapsed = time.time() - t0\n",
        "        kmeans_saving[f\"n_clusters{k}\"] = saving_from_kmeans(X_feat, prot_meta, km)\n",
        "        kmeans_saving[f\"time_k{k}\"]     = f\"{elapsed:.2f}s\"\n",
        "\n",
        "        # Pair-level eval\n",
        "        pairs = measure_pairwise_performance(kmeans_saving[f\"n_clusters{k}\"], X_feat)\n",
        "        naive_list, dist_list, one2one_list = pairs\n",
        "\n",
        "        def count_pairs(pairs_list, idx):\n",
        "            n_corr, n_tot = 0, len(pairs_list)\n",
        "            list_all_groups_no_set = [p[4] for p in prot_meta]\n",
        "            n_species = len(set([p[0] for p in prot_meta]))\n",
        "            for prots in pairs_list:\n",
        "                same_group = (len(set([p[4] for p in prots])) == 1)\n",
        "                if not same_group:\n",
        "                    continue\n",
        "                if idx == 2:\n",
        "                    # additional 1:1 check (same as original code)\n",
        "                    if list_all_groups_no_set.count(prots[0][4]) == n_species:\n",
        "                        n_corr += 1\n",
        "                else:\n",
        "                    n_corr += 1\n",
        "            return n_corr, n_tot\n",
        "\n",
        "        n_corr_naive, n_tot_naive = count_pairs(naive_list, 0)\n",
        "        n_corr_dist,  n_tot_dist  = count_pairs(dist_list, 1)\n",
        "        n_corr_121,   n_tot_121   = count_pairs(one2one_list, 2)\n",
        "\n",
        "        # Group-level eval\n",
        "        fam, ami, exact = measure_group_performance(kmeans_saving[f\"n_clusters{k}\"])\n",
        "\n",
        "        rows.append({\n",
        "            \"k\": k,\n",
        "            \"naive_correct\": n_corr_naive, \"naive_total\": n_tot_naive,\n",
        "            \"dist_correct\":  n_corr_dist,  \"dist_total\":  n_tot_dist,\n",
        "            \"one2one_correct\": n_corr_121, \"one2one_total\": n_tot_121,\n",
        "            \"family\": float(fam), \"AMI\": float(ami), \"exact_pct\": float(exact*100.0),\n",
        "            \"kmeans_time\": kmeans_saving[f\"time_k{k}\"]\n",
        "        })\n",
        "\n",
        "    # Save PKL (full kmeans_saving, includes meta inside each entry)\n",
        "    with open(os.path.join(EXP_DIR, f\"{TEST_SPEC}_{variant_tag}_{LOSS_TYPE}_kmeans.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(kmeans_saving, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    # Save CSV metrics\n",
        "    csv_path = os.path.join(EXP_DIR, f\"{TEST_SPEC}_{variant_tag}_{LOSS_TYPE}_metrics.csv\")\n",
        "    with open(csv_path, \"w\", newline=\"\") as f:\n",
        "        cw = csv.writer(f)\n",
        "        cw.writerow([\"k\",\n",
        "                     \"naive_correct\",\"naive_total\",\"naive_pct\",\n",
        "                     \"dist_correct\",\"dist_total\",\"dist_pct\",\n",
        "                     \"one2one_correct\",\"one2one_total\",\"one2one_pct\",\n",
        "                     \"family\",\"AMI\",\"exact_pct\",\"kmeans_time\"])\n",
        "        for r in rows:\n",
        "            cw.writerow([\n",
        "                r[\"k\"],\n",
        "                r[\"naive_correct\"], r[\"naive_total\"],\n",
        "                (100.0*r[\"naive_correct\"]/max(1,r[\"naive_total\"])) if r[\"naive_total\"] else 0.0,\n",
        "                r[\"dist_correct\"],  r[\"dist_total\"],\n",
        "                (100.0*r[\"dist_correct\"]/max(1,r[\"dist_total\"])) if r[\"dist_total\"] else 0.0,\n",
        "                r[\"one2one_correct\"], r[\"one2one_total\"],\n",
        "                (100.0*r[\"one2one_correct\"]/max(1,r[\"one2one_total\"])) if r[\"one2one_total\"] else 0.0,\n",
        "                r[\"family\"], r[\"AMI\"], r[\"exact_pct\"], r[\"kmeans_time\"]\n",
        "            ])\n",
        "    print(f\"[saved] {csv_path}\")\n",
        "    return rows\n",
        "\n",
        "# ---- Run for plain & top-k latents ----\n",
        "rows_plain = run_kmeans_eval(\n",
        "    Z_plain, prot_names_and_group_test,\n",
        "    variant_tag=f\"AE{LATENT_DIM}_{loss_tag}_{lam_tag}_plain\"\n",
        ")\n",
        "# if USE_TOPK:\n",
        "#     rows_topk  = run_kmeans_eval(Z_topk,  prot_names_and_group_test, variant_tag=f\"AE{LATENT_DIM}_topk{TOPK}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYSuKjOi5j0D",
        "outputId": "0697d7fc-7a13-4d12-ad2f-71d33a6ab27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE128_cosine_lam0.00075_plain] N=10263, #OGs=5315, ks=[5131]\n",
            "  k=5131\n",
            "[saved] /content/drive/MyDrive/ae_ablations/lamdaL1_7.5e-4/TRAIN[drer_xtro+mmus_hsap]_TEST[pfal_pber]_esm2_t36_3B_UR50D_AE128_L1-0.00075_SIG0.0_DO0.1_ninit50_iter500_topk0_20250913-144019_esm2_t36_3B_UR50D_AE128_loss-cosine_L1-0.00075_SIG0.0_DO0.1_ninit50_iter500_topk0_20250913-144019/pfal_pber_AE128_cosine_lam0.00075_plain_cosine_metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta = {\n",
        "  \"train_pool\": TRAIN_SPEC,\n",
        "  \"test\": TEST_SPEC,\n",
        "  \"model\": MODEL_NAME,\n",
        "  \"emb_layer\": EMB_LAYER,\n",
        "  \"ae\": {\n",
        "    \"latent_dim\": LATENT_DIM, \"lambda_l1\": LAMBDA_L1,\n",
        "    \"noise_sigma\": NOISE_SIGMA, \"dropout\": DROPOUT_P,\n",
        "    \"lr\": LR, \"weight_decay\": WD, \"batch_size\": BATCH_SIZE,\n",
        "    \"epochs\": EPOCHS, \"patience\": PATIENCE\n",
        "  },\n",
        "  \"kmeans\": {\n",
        "    \"n_init\": KMEANS_NINIT, \"max_iter\": KMEANS_MAXITER,\n",
        "    \"algorithm\": KMEANS_ALGO, \"random_state\": RAND_STATE\n",
        "  },\n",
        "  \"topk_enabled\": USE_TOPK, \"topk\": TOPK,\n",
        "  \"exp_dir\": EXP_DIR, \"timestamp\": ts, \"device\": str(device)\n",
        "}\n",
        "save_json(meta, os.path.join(EXP_DIR, \"experiment_meta.json\"))\n",
        "print(\"[saved] experiment_meta.json\")\n"
      ],
      "metadata": {
        "id": "RYS8Copj5mNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3b18174-3f74-423f-a8da-75e31795eb60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[saved] experiment_meta.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5vPcmEG0FA0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}